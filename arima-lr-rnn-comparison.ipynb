{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare TensorFlow RNN, ARIMA & Linear Regression Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate & Plot Test Data Set\n",
    "\n",
    "Also do all the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import util\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (16, 12)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "# Number of days to generate data for\n",
    "DAYS = 1\n",
    "\n",
    "# generate a numpy array of raw data first\n",
    "d = util.gen_data(days=DAYS)\n",
    "\n",
    "# turn it into a pandas data frame\n",
    "df = pd.DataFrame({'Time': d[:, 0], 'ACEs': d[:, 1]})\n",
    "\n",
    "# plot just the ACEs series\n",
    "# plt.plot(df['ACEs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# plot raw data using bokeh\n",
    "#\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "p = figure(width=1000)\n",
    "p.line(x=df['Time'], y=df['ACEs'], color='steelblue', line_width=2, legend_label=\"Test Data\")\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over Data & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of historical data points to start with\n",
    "HIST = 100\n",
    "\n",
    "# number of historical datapoints to consider for LR & ARIMA\n",
    "POINTS_ARIMA = 5\n",
    "POINTS_LR = 5\n",
    "POLY_ORDER = 1\n",
    "\n",
    "x = [x for x in range(0, 1440)]\n",
    "y = df['ACEs'].values\n",
    "\n",
    "x_train, x_test = x[:HIST], x[HIST:]\n",
    "y_train, y_test = y[:HIST], y[HIST:]\n",
    "\n",
    "history_x = list([x for x in x_train])\n",
    "history_y = list([y for y in y_train])\n",
    "\n",
    "predictions_lr = list()\n",
    "predictions_arima = list()\n",
    "predictions_rnn_single = list()\n",
    "predictions_rnn_multi = list()\n",
    "\n",
    "\n",
    "BREADTH = 30\n",
    "\n",
    "#\n",
    "# create and compile a single-layer model\n",
    "#\n",
    "# def create_model_single(breadth=BREADTH, input_shape=None):\n",
    "#     assert input_shape is not None\n",
    "#     retval = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.LSTM(BREADTH, input_shape=input_shape),\n",
    "#         tf.keras.layers.Dense(1)\n",
    "#     ])\n",
    "#     retval.compile(optimizer='adam', loss='mae')\n",
    "#     return retval\n",
    "\n",
    "#\n",
    "# create and compile a multi-layer model\n",
    "#\n",
    "# def create_model_multi(breadth=BREADTH, input_shape=None):\n",
    "#     assert input_shape is not None\n",
    "#     retval = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.LSTM(BREADTH, return_sequences=True, input_shape=input_shape),\n",
    "#         tf.keras.layers.LSTM(BREADTH, return_sequences=True),\n",
    "#         tf.keras.layers.LSTM(BREADTH),\n",
    "#         tf.keras.layers.Dense(1)\n",
    "#     ])\n",
    "#     retval.compile(optimizer='adam', loss='mae')\n",
    "#     return retval    \n",
    "\n",
    "#\n",
    "# load in normaliization parameters from test data\n",
    "#\n",
    "with open('mean_stddev.json', 'r') as f:\n",
    "    data = json.loads(f.read())\n",
    "TRAIN_MEAN = data['mean']\n",
    "TRAIN_STDDEV = data['stddev']\n",
    "\n",
    "#\n",
    "# instantiate a pre-trained RNNs\n",
    "#\n",
    "# generate a filename for weights based on these parameters\n",
    "LOAD_EPOCHS              = 5\n",
    "LOAD_EVALUATION_INTERVAL = 200\n",
    "LOAD_VALIDATION_STEPS    = 50\n",
    "\n",
    "# single-layer model with weights\n",
    "# model_rnn_single = create_model_single(input_shape=(BREADTH, 1))\n",
    "# model_rnn_single.load_weights('weights-prerun/lstm_weights-single-%03d-%03d-%03d.weights.h5' % (LOAD_EPOCHS, LOAD_EVALUATION_INTERVAL, LOAD_VALIDATION_STEPS))\n",
    "# model_rnn_single.load_weights('lstm_weights-single-%03d-%03d-%03d.weights.h5' % (LOAD_EPOCHS, LOAD_EVALUATION_INTERVAL, LOAD_VALIDATION_STEPS))\n",
    "\n",
    "# single layer model load\n",
    "model_rnn_single = tf.keras.models.load_model('lstm_model-single-%03d-%03d-%03d.keras' % (LOAD_EPOCHS, LOAD_EVALUATION_INTERVAL, LOAD_VALIDATION_STEPS))\n",
    "\n",
    "# multi-layer model\n",
    "# model_rnn_multi = create_model_multi(input_shape=(BREADTH, 1))\n",
    "#model_rnn_multi.load_weights('weights-prerun/lstm_weights-multi-%03d-%03d-%03d.weights.h5' % (LOAD_EPOCHS, LOAD_EVALUATION_INTERVAL, LOAD_VALIDATION_STEPS))\n",
    "# model_rnn_multi.load_weights('lstm_weights-multi-%03d-%03d-%03d.weights.h5' % (LOAD_EPOCHS, LOAD_EVALUATION_INTERVAL, LOAD_VALIDATION_STEPS))\n",
    "\n",
    "# multi-layer model\n",
    "model_rnn_multi = tf.keras.models.load_model('lstm_model-multi-%03d-%03d-%03d.keras' % (LOAD_EPOCHS, LOAD_EVALUATION_INTERVAL, LOAD_VALIDATION_STEPS))\n",
    "\n",
    "# loop over test array now\n",
    "for x, y in tqdm(zip(x_test, y_test)):\n",
    "    \n",
    "    # do ARIMA model & prediction\n",
    "    model_arima = ARIMA(history_y, order=(POINTS_ARIMA, 1, 0))\n",
    "    model_arima_fit = model_arima.fit()\n",
    "    output = model_arima_fit.forecast()\n",
    "    yhat_ar = output[0]\n",
    "    predictions_arima.append(yhat_ar)\n",
    "    \n",
    "    # do LR\n",
    "    x_arr = history_x[-POINTS_LR:]\n",
    "    y_arr = history_y[-POINTS_LR:]\n",
    "    fit = np.polyfit(x_arr, y_arr, POLY_ORDER, full=True)\n",
    "    yhat_lr = np.polyval(fit[0], x)\n",
    "    predictions_lr.append(yhat_lr)\n",
    "    \n",
    "    # do RNN predictions on last 30 data points\n",
    "    y_window = np.array(history_y[-BREADTH:])\n",
    "    y_arr = np.concatenate([np.roll(y_window, i) for i in range(0, 256)])\n",
    "    y_arr = (y_arr - TRAIN_MEAN) / TRAIN_STDDEV\n",
    "    y_arr = y_arr.reshape(-1, 30, 1)\n",
    "    \n",
    "    # single-layer RNN\n",
    "    yhat_rnn_single = model_rnn_single.predict(y_arr)[0][0] * TRAIN_STDDEV + TRAIN_MEAN\n",
    "    predictions_rnn_single.append(yhat_rnn_single)\n",
    "    \n",
    "    # multi-layer RNN\n",
    "    yhat_rnn_multi = model_rnn_multi.predict(y_arr)[0][0] * TRAIN_STDDEV + TRAIN_MEAN\n",
    "    predictions_rnn_multi.append(yhat_rnn_multi)\n",
    "    \n",
    "    # update history\n",
    "    history_x.append(x)\n",
    "    history_y.append(y)\n",
    "    \n",
    "\n",
    "#\n",
    "# does some MSE calculations\n",
    "#\n",
    "error_arima = mean_squared_error(y_test, predictions_arima)\n",
    "error_lr = mean_squared_error(y_test, predictions_lr)\n",
    "error_rnn_single = mean_squared_error(y_test, predictions_rnn_single)\n",
    "error_rnn_multi = mean_squared_error(y_test, predictions_rnn_multi)\n",
    "\n",
    "print('Test MSE (ARIMA)      : %.3f' % error_arima)\n",
    "print('Test MSE (LR)         : %.3f' % error_lr)\n",
    "print('Test MSE (RNN single) : %.3f' % error_rnn_single)\n",
    "print('Test MSE (RNN multi)  : %.3f' % error_rnn_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# plot same data using bokeh\n",
    "#\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "p = figure(min_width=1000, min_height=600)\n",
    "p.line(x=history_x[HIST:], y=history_y[HIST:], color='green', line_width=4, alpha=0.6, legend_label='True Data')\n",
    "p.line(x=history_x[HIST:], y=predictions_lr, color='blue', legend_label='LR Predictions')\n",
    "p.line(x=history_x[HIST:], y=predictions_arima, color='orange', legend_label='ARIMA Predictions')\n",
    "p.line(x=history_x[HIST:], y=predictions_rnn_single, color='red', legend_label='Single-Layer RNN', line_width=2)\n",
    "p.line(x=history_x[HIST:], y=predictions_rnn_multi, color='purple', legend_label='Multi-Layer RNN')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(history[HIST:], color='green')\n",
    "#plt.plot(predictions_arima, color='blue')\n",
    "#plt.plot(predictions_lr, color='orange')\n",
    "#plt.plot(predictions_rnn_single, color='red')\n",
    "#plt.plot(predictions_rnn_multi, color='purple')\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
