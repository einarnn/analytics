{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Time Series Predictions\n",
    "\n",
    "Play with the same data as linear regressions. Based on:\n",
    "\n",
    "- https://www.tensorflow.org/tutorials/structured_data/time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Basic Setup Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import util\n",
    "import datetime\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (16, 8)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "# we may optionally define this further down, but set to None right here to allow\n",
    "# it to be tested\n",
    "tensorboard_callback = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of days to generate data for\n",
    "DAYS = 10\n",
    "\n",
    "# generate a numpy array of raw data first\n",
    "d = util.gen_data(days=DAYS)\n",
    "\n",
    "# turn it into a pandas data frame\n",
    "df = pd.DataFrame({'Time': d[:, 0], 'ACEs': d[:, 1]})\n",
    "\n",
    "# plot just the ACEs series\n",
    "plt.plot(df['ACEs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Data Extraction\n",
    "\n",
    "This function takes in a 1D dataset of values (`dataset`) and carves it up into two different returns:\n",
    "\n",
    "- An array of arrays of length `history_size` of overlapping data, starting with the data at `start_index`, ending at `end_index`.\n",
    "- A simple array of future values that are `target_size` ticks in the future from each of the arrays above that are effectively the value we're trying to train to/for.\n",
    "\n",
    "So, for setting `target_size`, consider how far into the future you want to predict, and for `history_size` consider how much of past history you want to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
    "    '''\n",
    "    * dataset the 1D array of data\n",
    "    * start_index where in dataset to get data from\n",
    "    * end_index last index to take data from\n",
    "    * history_size size of past window of information\n",
    "    * target_size how far in the future to predict\n",
    "    '''\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, 1)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        labels.append(dataset[i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data = df['ACEs']\n",
    "uni_data.index = df['Time']\n",
    "uni_data.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_train_mean = uni_data.mean()\n",
    "uni_train_std = uni_data.std()\n",
    "\n",
    "print('Mean    = {}'.format(uni_train_mean))\n",
    "print('Std Dev = {}'.format(uni_train_std))\n",
    "\n",
    "# normalize all elements\n",
    "uni_data = (uni_data-uni_train_mean)/uni_train_std\n",
    "    \n",
    "uni_data.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uni_data_vals = uni_data.values\n",
    "\n",
    "# uses the past \"day\" for history (one tick is one minute)\n",
    "# univariate_past_history = 1440\n",
    "\n",
    "# try 30 minutes of history\n",
    "univariate_past_history = 30\n",
    "\n",
    "# predict 1 minute ahead\n",
    "univariate_future_target = 0\n",
    "\n",
    "x_train_uni, y_train_uni = univariate_data(uni_data_vals, 0, 1440*(DAYS-2),\n",
    "                                           univariate_past_history,\n",
    "                                           univariate_future_target)\n",
    "x_val_uni, y_val_uni = univariate_data(uni_data_vals, 1440*(DAYS-2), None,\n",
    "                                       univariate_past_history,\n",
    "                                       univariate_future_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x_train_uni.shape)\n",
    "print(x_train_uni)\n",
    "print(y_train_uni.shape)\n",
    "print(y_train_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('Single window of past history')\n",
    "print (x_train_uni[0])\n",
    "print ('\\n Target hardware resource utilization to predict (normalized)')\n",
    "print (y_train_uni[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_steps(length):\n",
    "    return list(range(-length, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Plot data, depending on its shape , and add in the \"true future\" as a green cross,\n",
    "# and a prediction, if available, as a red dot\n",
    "#\n",
    "# The shape here is defined by the `univariate_past_history` defined earlier\n",
    "#\n",
    "def show_plot(plot_data, delta, title):\n",
    "    labels = ['History', 'True Future', 'Model Prediction']\n",
    "    marker = ['.-', 'gx', 'ro']\n",
    "    time_steps = create_time_steps(plot_data[0].shape[0])\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, x in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future+5)*2])\n",
    "    plt.xlabel('Time-Step')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_plot([x_train_uni[0], y_train_uni[0]], 0, 'Sample Example').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot([x_train_uni[500], y_train_uni[500]], 0, 'Sample Example').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Average of Last Day of Observations & Predict\n",
    "\n",
    "As best practice, always good to take a simple baseline prediction and check that whatever you do with respect to ML improves on that! In this case, take a simple everage of one of the training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(history):\n",
    "    return np.mean(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot([x_train_uni[500], y_train_uni[500], baseline(x_train_uni[500])], 0, 'Baseline Prediction Example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really not a very good prediction mechanism!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train A Recurrent Neural Network\n",
    "\n",
    "First, look at the basic shape of the data we're using for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_uni.shape)\n",
    "print(y_train_uni.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Up The Data Into TF Data Structures\n",
    "\n",
    "Not going to go into this in detail, but first we need to split the training data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = x_train_uni.shape[0]\n",
    "\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
    "val_univariate = val_univariate.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile The Model\n",
    "\n",
    "First, define a function to create and compile the model. We use \"LSTM\" (Long Short-Term Memory\") cells as time series predictions need to learn from past observartions to predict the next value in the sequence.\n",
    "\n",
    "First we define a coiuple of functions to create models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BREADTH = 30\n",
    "\n",
    "#\n",
    "# create and compile a single-layer model\n",
    "#\n",
    "def create_model_single(breadth=BREADTH, input_shape=None):\n",
    "    assert input_shape is not None\n",
    "    retval = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(BREADTH, input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    retval.compile(optimizer='adam', loss='mae')\n",
    "    return retval\n",
    "\n",
    "#\n",
    "# create and compile a multi-layer model\n",
    "#\n",
    "def create_model_multi(breadth=BREADTH, input_shape=None):\n",
    "    assert input_shape is not None\n",
    "    retval = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(BREADTH, return_sequences=True, input_shape=input_shape),\n",
    "        tf.keras.layers.LSTM(BREADTH, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(BREADTH),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    retval.compile(optimizer='adam', loss='mae')\n",
    "    return retval    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_lstm_model = create_model_single(input_shape=x_train_uni.shape[-2:])\n",
    "multi_lstm_model  = create_model_multi(input_shape=x_train_uni.shape[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at what a prediction will give you by taking one window from the validation set and calling `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_univariate.take(1):\n",
    "    print(single_lstm_model.predict(x).shape)\n",
    "    print(multi_lstm_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model if we want. But skip this if you just want to load pre-created weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 200  # original 200\n",
    "VALIDATION_STEPS    = 50   # original 50\n",
    "EPOCHS              = 10   # original 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for logging data for TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_callback(l=None, evaluations=None, epochs=None, validations=None):\n",
    "    assert l is not None\n",
    "    assert evaluations is not None\n",
    "    assert epochs is not None\n",
    "    assert validations is not None\n",
    "    \n",
    "    log_dir = \"logs/fit/\" + \\\n",
    "              \"%s-%s-%03d-%03d-%03d\" % (datetime.datetime.now().strftime(\"%m%d-%H%M\"), l, epochs, evaluations, validations)\n",
    "    return tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Single Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_lstm_single = single_lstm_model.fit(\n",
    "    train_univariate,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=EVALUATION_INTERVAL,\n",
    "    validation_data=val_univariate,\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    callbacks=[tensorboard_callback(l='single', evaluations=EVALUATION_INTERVAL, epochs=EPOCHS, validations=VALIDATION_STEPS)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Multi-Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_lstm_multi = multi_lstm_model.fit(\n",
    "    train_univariate,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=EVALUATION_INTERVAL,\n",
    "    validation_data=val_univariate,\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    callbacks=[tensorboard_callback(l='multi', evaluations=EVALUATION_INTERVAL, epochs=EPOCHS, validations=VALIDATION_STEPS)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model Weights & Normalization  Factors For Reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the weights. Again, don't do this if we're loading a predefined set of weights. We also need to save the normalization factors we calculated over the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_lstm_model.save_weights(\n",
    "    'lstm_weights-%s-%03d-%03d-%03d' % ('single', EPOCHS, EVALUATION_INTERVAL, VALIDATION_STEPS))\n",
    "multi_lstm_model.save_weights(\n",
    "    'lstm_weights-%s-%03d-%03d-%03d' % ('multi', EPOCHS, EVALUATION_INTERVAL, VALIDATION_STEPS))\n",
    "\n",
    "# save out the mean & stddev for use later in another notebook\n",
    "with open('mean_stddev.json', 'w', encoding='utf-8') as f:\n",
    "    data = {\n",
    "        'mean': uni_train_mean,\n",
    "        'stddev': uni_train_std\n",
    "    }\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models From Pre-Defined Weights Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a predefined set of weights. Note that the model definition really needs top be the same, so look out for that, noting that some weights files will load into a model that is not the same; seems to happen when you have more layers in the model you trained than you are loading into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a filename for weights based on these parameters\n",
    "LOAD_EPOCHS = 30\n",
    "LOAD_EVALUATION_INTERVAL = 100\n",
    "LOAD_VALIDATION_STEPS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-layer model\n",
    "loaded_single_lstm_model = create_model_single(input_shape=x_train_uni.shape[-2:])\n",
    "loaded_single_lstm_model.load_weights('weights-prerun/lstm_weights-single-%03d-%03d-%03d' % (LOAD_EPOCHS, LOAD_EVALUATION_INTERVAL, LOAD_VALIDATION_STEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-layer model\n",
    "loaded_multi_lstm_model = create_model_multi(input_shape=x_train_uni.shape[-2:])\n",
    "loaded_multi_lstm_model.load_weights('weights-prerun/lstm_weights-multi-%03d-%03d-%03d' % (LOAD_EPOCHS, LOAD_EVALUATION_INTERVAL, LOAD_VALIDATION_STEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_single_lstm_model.summary()\n",
    "loaded_multi_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Some Sample Predictions\n",
    "\n",
    "This loop will display a plot per prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for x, y in val_univariate.take(25):\n",
    "    plot = show_plot([x[0].numpy(), y[0].numpy(), loaded_single_lstm_model.predict(x)[0]], 0, 'Single-Layer LSTM model')\n",
    "    plot.show()\n",
    "    plot = show_plot([x[0].numpy(), y[0].numpy(), loaded_multi_lstm_model.predict(x)[0]], 0, 'Multi-Layer LSTM model')\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Mean Squared Errors\n",
    "\n",
    "Simnple MSE calculation against a random selection of the validation data created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# lstm_model = simple_lstm_model\n",
    "lstm_model = loaded_lstm_model\n",
    "\n",
    "real = []\n",
    "predictions = []\n",
    "for x, y in tqdm(val_univariate.take(250)):\n",
    "    \n",
    "    # print(x.shape)\n",
    "    \n",
    "    # predicted value, scaled back up\n",
    "    predictions.append(lstm_model.predict(x)[0][0] * uni_train_std + uni_train_mean)\n",
    "    \n",
    "    # real value, scaled back up\n",
    "    real.append(y[0].numpy() * uni_train_std + uni_train_mean)\n",
    "\n",
    "error = mean_squared_error(real, predictions)\n",
    "print('Test MSE : %.9f' % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df\n",
    "start_index = 0\n",
    "end_index = 1440 * 3\n",
    "history_size = 1440\n",
    "target_size = 0\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "start_index = start_index + history_size\n",
    "if end_index is None:\n",
    "    end_index = len(dataset) - target_size\n",
    "\n",
    "for i in range(start_index, end_index):\n",
    "    print('i', i)\n",
    "    indices = range(i-history_size, i)\n",
    "    print('indices', indices)\n",
    "    # Reshape data from (history_size,) to (history_size, 1)\n",
    "    data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "    labels.append(dataset[i+target_size])\n",
    "\n",
    "np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Time']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
